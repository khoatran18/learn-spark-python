{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:29.772387Z",
     "start_time": "2025-10-20T10:06:29.765623Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, LongType, DateType, TimestampType\n",
    "from pyspark.sql.functions import upper, pandas_udf, expr\n",
    "\n",
    "from datetime import datetime, date\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:29.832025Z",
     "start_time": "2025-10-20T10:06:29.820528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import os, sys\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\Lenovo\\miniconda3\\python.exe\"\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = r\"C:\\Users\\Lenovo\\miniconda3\\python.exe\"\n",
    "\n",
    "\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PandasToSparkTest\")\n",
    "    .master(\"local[*]\")  # dùng toàn bộ CPU\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.python.worker.reuse\", \"false\")   # tránh lỗi worker treo\n",
    "    .config(\"spark.network.timeout\", \"300s\")\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enable\", True)"
   ],
   "id": "269a8fd4946e75a0",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:29.910351Z",
     "start_time": "2025-10-20T10:06:29.875385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c=\"string1\", d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c=\"string2\", d=date(2000, 1, 2), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=3, b=4., c=\"string3\", d=date(2000, 1, 3), e=datetime(2000, 3, 1, 12, 0)),\n",
    "])\n",
    "\n",
    "df\n"
   ],
   "id": "5236065282ef304c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:30.035028Z",
     "start_time": "2025-10-20T10:06:30.007568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [\n",
    "    Row(a=1, b=2., c=\"string1\", d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c=\"string2\", d=date(2000, 1, 2), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=3, b=4., c=\"string3\", d=date(2000, 1, 3), e=datetime(2000, 3, 1, 12, 0)),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"a\", LongType(), True),\n",
    "    StructField(\"b\", DoubleType(), True),\n",
    "    StructField(\"c\", StringType(), True),\n",
    "    StructField(\"d\", DateType(), True),\n",
    "    StructField(\"e\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df\n"
   ],
   "id": "ab9db732944063ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:30.160176Z",
     "start_time": "2025-10-20T10:06:30.128443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create spark DataFream from pandas\n",
    "\n",
    "pd_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "\n",
    "df = spark.createDataFrame(pd_df)\n",
    "\n",
    "df\n"
   ],
   "id": "4926c2ae24972fa2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:30.402375Z",
     "start_time": "2025-10-20T10:06:30.333220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.show()\n",
    "df.show(1, vertical=True)\n",
    "df.printSchema()\n",
    "df.columns\n",
    "\n",
    "print(\"Test new 1\")\n",
    "df.select(df.a, \"b\", \"c\").describe()\n",
    "# df.select(\"a\", \"b\", \"c\").describe().show()"
   ],
   "id": "83698d0d812cb3b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | string1             \n",
      " d   | 2000-01-01          \n",
      " e   | 2000-01-01 12:00:00 \n",
      "only showing top 1 row\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n",
      "Test new 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, a: string, b: string, c: string]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:30.498942Z",
     "start_time": "2025-10-20T10:06:30.482766Z"
    }
   },
   "cell_type": "code",
   "source": "df.limit(2).collect()\n",
   "id": "fc6666dff7553183",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0))]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:30.857022Z",
     "start_time": "2025-10-20T10:06:30.850670Z"
    }
   },
   "cell_type": "code",
   "source": "type(df.c.isNull())",
   "id": "bc77b7eb375d7773",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.classic.column.Column"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:31.049957Z",
     "start_time": "2025-10-20T10:06:31.011722Z"
    }
   },
   "cell_type": "code",
   "source": "df.withColumn('upper_c', upper(df.c)).filter(df.a == 1).show()",
   "id": "fd54186b5c8fe232",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  a|  b|      c|         d|                  e|upper_c|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:31.815147Z",
     "start_time": "2025-10-20T10:06:31.254818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pandas_udf('long')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    return series + 1\n",
    "\n",
    "df.select(pandas_plus_one(df.a)).show()"
   ],
   "id": "c8ec0983e59017a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|pandas_plus_one(a)|\n",
      "+------------------+\n",
      "|                 2|\n",
      "|                 3|\n",
      "|                 4|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:32.501989Z",
     "start_time": "2025-10-20T10:06:32.479464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pandas_filter_func(iterator):\n",
    "    for pandas_df in iterator:\n",
    "        yield pandas_df[pandas_df.a == 1]\n",
    "\n",
    "df.mapInPandas(pandas_filter_func, schema=df.schema)"
   ],
   "id": "146fd57d2fb70455",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:33.278023Z",
     "start_time": "2025-10-20T10:06:32.692829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dfn = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "dfn.show()"
   ],
   "id": "2a7fce24668fd1d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:33.911952Z",
     "start_time": "2025-10-20T10:06:33.510195Z"
    }
   },
   "cell_type": "code",
   "source": "dfn.groupBy('color').avg().show()",
   "id": "ca454753823533c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|avg(v1)|avg(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|    4.8|   48.0|\n",
      "| blue|    3.0|   30.0|\n",
      "|black|    6.0|   60.0|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:35.088523Z",
     "start_time": "2025-10-20T10:06:34.314371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plus_mean(pandas_df):\n",
    "    return pandas_df.assign(v1 = pandas_df.v1 - pandas_df.v1.mean())\n",
    "\n",
    "dfn.groupBy('color').applyInPandas(plus_mean, schema=dfn.schema).show()"
   ],
   "id": "88b447d259540cb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  0| 60|\n",
      "| blue|banana| -1| 20|\n",
      "| blue| grape|  1| 40|\n",
      "|  red|banana| -3| 10|\n",
      "|  red|carrot| -1| 30|\n",
      "|  red|carrot|  0| 50|\n",
      "|  red|banana|  2| 70|\n",
      "|  red| grape|  3| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:36.689121Z",
     "start_time": "2025-10-20T10:06:35.689315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    ('time', 'id', 'v1'))\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n",
    "    ('time', 'id', 'v2'))\n",
    "\n",
    "def merge_order(l, r):\n",
    "    return pd.merge_ordered(l, r)\n",
    "\n",
    "df1.groupBy('id').cogroup(df2.groupBy('id')).applyInPandas(merge_order, schema='time int, id int, v1 double, v2 string').show()"
   ],
   "id": "a4871062d6284879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+----+\n",
      "|    time| id| v1|  v2|\n",
      "+--------+---+---+----+\n",
      "|20000101|  1|1.0|   x|\n",
      "|20000102|  1|3.0|NULL|\n",
      "|20000101|  2|2.0|   y|\n",
      "|20000102|  2|4.0|NULL|\n",
      "+--------+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:37.156348Z",
     "start_time": "2025-10-20T10:06:37.016610Z"
    }
   },
   "cell_type": "code",
   "source": "df.write.csv(\"foo.csv\", header=True)",
   "id": "b489edaf00070d3f",
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/home/tmkhoa-1812/Projects/PyCharm/LearnPySpark/learn-spark-python/spark/foo.csv already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAnalysisException\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[166]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrite\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfoo.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheader\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/pyspark3.12/lib/python3.12/site-packages/pyspark/sql/readwriter.py:2146\u001B[39m, in \u001B[36mDataFrameWriter.csv\u001B[39m\u001B[34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[39m\n\u001B[32m   2127\u001B[39m \u001B[38;5;28mself\u001B[39m.mode(mode)\n\u001B[32m   2128\u001B[39m \u001B[38;5;28mself\u001B[39m._set_opts(\n\u001B[32m   2129\u001B[39m     compression=compression,\n\u001B[32m   2130\u001B[39m     sep=sep,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2144\u001B[39m     lineSep=lineSep,\n\u001B[32m   2145\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m2146\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jwrite\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/pyspark3.12/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1356\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1357\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1358\u001B[39m     args_command +\\\n\u001B[32m   1359\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1361\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1362\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1363\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1365\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1366\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/pyspark3.12/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    284\u001B[39m converted = convert_exception(e.java_exception)\n\u001B[32m    285\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[32m    286\u001B[39m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[32m    287\u001B[39m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m288\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    289\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    290\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mAnalysisException\u001B[39m: [PATH_ALREADY_EXISTS] Path file:/home/tmkhoa-1812/Projects/PyCharm/LearnPySpark/learn-spark-python/spark/foo.csv already exists. Set mode as \"overwrite\" to overwrite the existing path. SQLSTATE: 42K04"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:37.489030309Z",
     "start_time": "2025-10-20T08:45:24.467300Z"
    }
   },
   "cell_type": "code",
   "source": "spark.read.csv(\"foo.csv\", header=True).show()",
   "id": "7c12532b0f833738",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+--------------------+\n",
      "|  a|  b|      c|         d|                   e|\n",
      "+---+---+-------+----------+--------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01T12:00:...|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02T12:00:...|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03T12:00:...|\n",
      "+---+---+-------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:37.505222652Z",
     "start_time": "2025-10-20T09:01:22.756559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.createOrReplaceTempView(\"tableA\")\n",
    "spark.sql(\"select count(*) from tableA\").show()"
   ],
   "id": "e2069b7864bc0625",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:37.587675119Z",
     "start_time": "2025-10-20T09:05:50.421867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@pandas_udf(\"integer\")\n",
    "def add_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "spark.udf.register(\"add_one\", add_one)\n",
    "spark.sql(\"select add_one(a) from tableA\").show()"
   ],
   "id": "a78fee8934935827",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|add_one(a)|\n",
      "+----------+\n",
      "|         2|\n",
      "|         3|\n",
      "|         4|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:37.605987470Z",
     "start_time": "2025-10-20T09:55:55.978248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.selectExpr('add_one(a)', 'b').show()\n",
    "df.select(expr('count(*)') > 0).show()\n",
    "df.show()"
   ],
   "id": "8d32d89602bfb04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|add_one(a)|  b|\n",
      "+----------+---+\n",
      "|         2|2.0|\n",
      "|         3|3.0|\n",
      "|         4|4.0|\n",
      "+----------+---+\n",
      "\n",
      "+--------------+\n",
      "|(count(1) > 0)|\n",
      "+--------------+\n",
      "|          true|\n",
      "+--------------+\n",
      "\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T10:06:37.610207905Z",
     "start_time": "2025-10-20T09:57:28.208714Z"
    }
   },
   "cell_type": "code",
   "source": "df.filter(expr(\"b > 2\")).select(expr(\"a + 1 as a1\"), expr(\"b as b1\"), expr('c as c1')).show()",
   "id": "6ac5b1687609263d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "| a1| b1|     c1|\n",
      "+---+---+-------+\n",
      "|  3|3.0|string2|\n",
      "|  4|4.0|string3|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 150
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
